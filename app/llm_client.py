# app/llm_client.py

from typing import List, Dict
from openai import AsyncOpenAI

from app.config import PROXYAPI_API_KEY

if not PROXYAPI_API_KEY:
    raise RuntimeError("PROXYAPI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø–æ–ª–Ω–∏ –µ–≥–æ –≤ .env")

# –ö–ª–∏–µ–Ω—Ç ProxyAPI (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç)
# –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –º–æ–¥–µ–ª—è–º –∏ —Ü–µ–Ω–∞–º —Å–º–æ—Ç—Ä–∏ —É ProxyAPI.
client = AsyncOpenAI(
    api_key=PROXYAPI_API_KEY,
    base_url="https://openai.api.proxyapi.ru/v1",
)

# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gpt-4o-mini, –ø–æ—Ç–æ–º –ø–æ–º–µ–Ω—è–µ–º –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏
DEFAULT_MODEL_NAME = "gpt-4o-mini"

DEFAULT_SYSTEM_PROMPT = (
    "–¢—ã —É–º–Ω—ã–π, –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç—É–¥–∏–∏ AINOVA. "
    "–û—Ç–≤–µ—á–∞–µ—à—å –ø–æ-–¥–µ–ª—É, –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –∏–Ω–æ–≥–¥–∞ –¥–∞—ë—à—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–≤–µ—Ç—ã –±–∏–∑–Ω–µ—Å—É. "
    "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ, –æ—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ."
)


async def ask_llm(
    messages: List[Dict[str, str]],
    model: str = DEFAULT_MODEL_NAME,
    temperature: float = 0.7,
) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM —á–µ—Ä–µ–∑ ProxyAPI.
    messages ‚Äî —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Ñ–æ—Ä–º–∞—Ç–∞:
    [{"role": "system" | "user" | "assistant", "content": "..."}, ...]
    """
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=False,
        )
        return response.choices[0].message.content
    except Exception as e:
        # –í—Ä–µ–º–µ–Ω–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM —á–µ—Ä–µ–∑ ProxyAPI:", repr(e))
        return "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ AI. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ üôè"

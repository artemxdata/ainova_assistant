# app/llm_client.py

from typing import Dict, List, Optional

from openai import AsyncOpenAI

from app.config import (
    PROXYAPI_API_KEY,
    PROXYAPI_BASE_URL,
    LLM_MODEL,
    LLM_TEMPERATURE,
)

_client: Optional[AsyncOpenAI] = None


def get_client() -> AsyncOpenAI:
    """
    –õ–µ–Ω–∏–≤–æ —Å–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç–∞ (–Ω–µ –ø–∞–¥–∞–µ–º –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ).
    """
    global _client
    if _client is not None:
        return _client

    if not PROXYAPI_API_KEY:
        raise RuntimeError("PROXYAPI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø–æ–ª–Ω–∏ –µ–≥–æ –≤ .env")

    _client = AsyncOpenAI(
        api_key=PROXYAPI_API_KEY,
        base_url=PROXYAPI_BASE_URL,
    )
    return _client


async def ask_llm(messages: List[Dict[str, str]]) -> str:
    """
    –ó–∞–ø—Ä–æ—Å –∫ LLM —á–µ—Ä–µ–∑ ProxyAPI (OpenAI-compatible).
    –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏/—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –±–µ—Ä—ë–º –∏–∑ config.
    """
    try:
        client = get_client()
        response = await client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            stream=False,
        )
        content = response.choices[0].message.content
        return content or ""
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM —á–µ—Ä–µ–∑ ProxyAPI:", repr(e))
        return "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ AI. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ üôè"
